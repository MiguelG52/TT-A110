{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fast-langdetect wordsegment fastapi gpt4all\n",
    "# Entorno tt_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALACIONES PARA NUEVO AMBIENTE\n",
    "# %pip install wordsegment\n",
    "# %pip install transformers datasets torch pandas scikit-learn\n",
    "# %pip install sentencepiece\n",
    "# %pip install protobuf\n",
    "# %pip install ipywidgets\n",
    "# %pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED NEURONAL\n",
    "\n",
    "En esta sección debido al desorden se empieza a trabajar en el modelo para sugerir el nombre de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file_path = \"./dataset/java_dataset.csv\"\n",
    "# df = pd.read_csv(file_path, encoding=\"latin-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\rod_e\\miniconda3\\envs\\tt_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    # T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    # T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 1. Cargar el dataset desde un archivo CSV\n",
    "file_path = \"./dataset/java_dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"latin-1\")\n",
    "\n",
    "# Asegurar que las columnas clave existen\n",
    "assert \"code\" in df.columns and \"suggested_name\" in df.columns, (\n",
    "    \"El CSV debe contener 'code' y 'suggested_name'\"\n",
    ")\n",
    "\n",
    "# 2. Tokenización con CodeT5\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    \"\"\"Función para tokenizar código y nombres sugeridos\"\"\"\n",
    "    inputs = [f\"Generate name: {code}\" for code in examples[\"code\"]]\n",
    "    targets = examples[\"suggested_name\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(targets, max_length=20, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Convertir DataFrame a Dataset de Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Aplicar tokenización al dataset\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# 3. Dividir en train/test\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "# 4. Cargar el modelo CodeT5\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "\n",
    "# 5. Configurar entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codet5-fine-tuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# 7. Guardar el modelo\n",
    "model.save_pretrained(\"./codet5-fine-tuned\")\n",
    "tokenizer.save_pretrained(\"./codet5-fine-tuned\")\n",
    "\n",
    "# print(\"¡Entrenamiento completado y modelo guardado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# simply generate a single sequence\n",
    "generated_ids = model.generate(input_ids, max_length=10)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "# this prints \"user: {user.name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from wordsegment import load, segment\n",
    "\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_datos = [\n",
    "    \"boolean\",\n",
    "    \"char\",\n",
    "    \"byte\",\n",
    "    \"short\",\n",
    "    \"int\",\n",
    "    \"long\",\n",
    "    \"float\",\n",
    "    \"double\",\n",
    "    \"String\",\n",
    "    \"Array\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    package com.hmkcode;\n",
    "\n",
    "import java.io.BufferedReader;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.FileReader;\n",
    "import java.io.IOException;\n",
    "\n",
    "public class textfilereader {\n",
    "\n",
    "\tprivate BufferedReader buffer;\n",
    "\tprivate String currentLine = \"\";\n",
    "\tpublic textfilereader(){ \n",
    "\t\t\n",
    "\t}\n",
    "\t\n",
    "\tpublic void open(String file){\n",
    "\t\t\n",
    "\t\ttry {\n",
    "\t\t\tclose();\n",
    "\t\t\t\n",
    "\t\t\tbuffer = new BufferedReader(new FileReader(file));\n",
    "\t\t\t\n",
    "\t\t} catch (FileNotFoundException e1) {\n",
    "\t\t\te1.printStackTrace();\n",
    "\t\n",
    "\t\t}\n",
    "\t\t\n",
    "\t}\n",
    "\t\n",
    "\tpublic void close(){\n",
    "\t\t\n",
    "\t\ttry {\n",
    "\t\t\tif(buffer != null){\n",
    "\t\t\t\tbuffer.close();\n",
    "\t\t\t\tbuffer = null;\n",
    "\t\t\t}\n",
    "\t\t} catch (IOException e) {\n",
    "\t\t\te.printStackTrace();\n",
    "\t\t}\n",
    "\t\t\n",
    "\t}\n",
    "\t\n",
    "\tpublic String readLine() throws Exception{\n",
    "\t\tif(buffer != null){\n",
    "\t\t\tcurrentLine = buffer.readLine();\n",
    "\t\t\t\n",
    "\t\t\tif(currentLine == null)\n",
    "\t\t\t\tclose();\n",
    "\t\t\t\n",
    "\t\t\treturn currentLine;\n",
    "\t\t}\n",
    "\t\telse\n",
    "\t\t\tthrow new Exception(\"No file to read...\");\n",
    "\t}\n",
    "\t\n",
    "\tpublic String getCurrent(){\n",
    "\t\treturn this.currentLine;\n",
    "\t}\n",
    "\t\n",
    "\tpublic boolean isReadable(){\n",
    "\t\treturn (buffer != null && this.currentLine != null);\n",
    "\t}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaración de regex para uso general\n",
    "class_starter = r\"(class)\\s([^\\{]+)\\{\"\n",
    "funciones_static = r\"(static\\s){1,1}.{0,}(){1,1}\"\n",
    "funciones_private = r\"(private\\s){1,1}.{0,}(){1,1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_coin = re.findall(pattern=class_starter, string=text)\n",
    "class_coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos el nombramiento de la clase\n",
    "if bool(class_coin):\n",
    "    # Con una expresión regular verificamos el nombramiento de la clase y su respectivo nombre\n",
    "    identificador = class_coin[0][0].strip()\n",
    "\n",
    "    t_t = text.split(\"\\n\")\n",
    "    n = len(t_t)\n",
    "\n",
    "    # Iteramos por cada linea de código hasta que terminamos con el proceso de cambiar el nombre de la clase por la forma de cammel case\n",
    "    for i in range(n):\n",
    "        if identificador in t_t[i]:\n",
    "            sep_l = t_t[i].split()\n",
    "            n_2 = len(sep_l)\n",
    "            for j in range(n_2):\n",
    "                if sep_l[j] != identificador and sep_l[j] != \"public\":\n",
    "                    sep_l[j] = separate_words(sep_l[j])\n",
    "                    break\n",
    "            sep_l = \" \".join(sep_l)\n",
    "            t_t[i] = sep_l\n",
    "text = \"\\n\".join(t_t)\n",
    "\n",
    "# De momento solo se revisa si hay nombramiento de metodos privados\n",
    "if \"private\" in text:\n",
    "    coincidencias = re.findall(pattern=funciones_private, string=text)\n",
    "\n",
    "    if bool(coincidencias):\n",
    "        # print(\"segundo if\")\n",
    "        c = coincidencias[0][0].strip()\n",
    "        t_l = text.split(\"\\n\")\n",
    "        for i in range(len(t_l)):\n",
    "            # print(c,line)\n",
    "            if c in t_l[i]:\n",
    "                # print(\"antes del title\", t_l[i].strip())\n",
    "                sep = t_l[i].split()\n",
    "                # print(sep.index(c))\n",
    "                for j in range(len(sep)):\n",
    "                    if sep[j] != c and sep[j] not in tipo_datos:\n",
    "                        sep[j] = separate_words(sep[j])\n",
    "                        break\n",
    "                sep = \" \".join(sep)\n",
    "                t_l[i] = sep\n",
    "text = \"\\n\".join(t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb_code = \"\"\"\n",
    "import java.sql.Connection;\n",
    "import java.sql.DriverManager;\n",
    "import java.sql.ResultSet;\n",
    "import java.sql.SQLException;\n",
    "import java.sql.Statement;\n",
    "\n",
    "public class DatabaseConnection {\n",
    "    private static final String URL = \"jdbc:mysql://localhost:3306/tu_base_de_datos\";\n",
    "    private static final String USER = \"tu_usuario\";\n",
    "    private static final String PASSWORD = \"tu_contraseña\";\n",
    "\n",
    "    private Connection connection;\n",
    "\n",
    "    public DatabaseConnection() {\n",
    "        try {\n",
    "            // Cargar el driver de MySQL\n",
    "            Class.forName(\"com.mysql.cj.jdbc.Driver\");\n",
    "            // Establecer la conexión\n",
    "            connection = DriverManager.getConnection(URL, USER, PASSWORD);\n",
    "            System.out.println(\"Conexión exitosa a la base de datos.\");\n",
    "        } catch (ClassNotFoundException | SQLException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public void ejecutarConsulta(String consulta) {\n",
    "        try (Statement statement = connection.createStatement();\n",
    "             ResultSet resultSet = statement.executeQuery(consulta)) {\n",
    "            while (resultSet.next()) {\n",
    "                System.out.println(\"Resultado: \" + resultSet.getString(1));\n",
    "            }\n",
    "        } catch (SQLException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public void cerrarConexion() {\n",
    "        try {\n",
    "            if (connection != null && !connection.isClosed()) {\n",
    "                connection.close();\n",
    "                System.out.println(\"Conexión cerrada.\");\n",
    "            }\n",
    "        } catch (SQLException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        DatabaseConnection db = new DatabaseConnection();\n",
    "        db.ejecutarConsulta(\"SELECT * FROM tu_tabla\");\n",
    "        db.cerrarConexion();\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_coin = re.findall(pattern=class_starter, string=dtb_code)\n",
    "class_coin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_camell_case(word: str) -> bool:\n",
    "    \"\"\"Función para determinar si la case ya esta en formato camell case, de ser así retorna un True\n",
    "\n",
    "    Args:\n",
    "        word (str): Nombre de la clase o el método\n",
    "\n",
    "    Returns:\n",
    "        bool: Valor booleano acorde a si es cammel case o no\n",
    "    \"\"\"\n",
    "    word = word.strip()\n",
    "\n",
    "    sep = segment(word)\n",
    "\n",
    "    final = \"\".join([item.capitalize() for item in sep])\n",
    "\n",
    "    if final.strip() == word:\n",
    "        return True\n",
    "    elif final.strip() != word:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_camell_case(class_coin[0][1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model_name=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\",\n",
    "    model_path=\"./models\",\n",
    "    verbose=True,\n",
    "    allow_download=False,\n",
    ")\n",
    "\n",
    "with llm.chat_session():\n",
    "    print(\n",
    "        llm.generate(\n",
    "            f\"Dime si este código de java cumple con las buenas prácticas de nombramiento del libro de clean code de Robert C. Martin: {dtb_code}, si las variables no cumplen con ser lo suficientemente descriptivas, retorna un diccionario de python donde las llaves sean los nombres de las variables actuales y el valor sea el nuevo nombre. No me expliques nada, devuelve unicamente el diccionario sin dar explicaciones ni agregar ni un comentario\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\rod_e\\miniconda3\\envs\\tt_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    # T5Tokenizer,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 1. Cargar el dataset desde el archivo CSV\n",
    "df = pd.read_csv(\"./dataset/dataset_final.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Se cambia el valor para trabajar con el\n",
    "df[\"is_correct\"] = df[\"is_correct\"].astype(str)\n",
    "\n",
    "df = df.dropna()  # Se eliminan 4 nulos, que son muy pocos comparados con los demás\n",
    "\n",
    "# Asegurar que las columnas clave existen\n",
    "assert (\n",
    "    \"code\" in df.columns\n",
    "    and \"suggested_name\" in df.columns\n",
    "    and \"type\" in df.columns\n",
    "    and \"is_correct\" in df.columns\n",
    "), \"El CSV debe contener 'code', 'suggested_name', 'type' e 'is_correct'\"\n",
    "\n",
    "# 2. Tokenización con CodeT5\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "\n",
    "# def preprocess_data(examples):\n",
    "#     \"\"\"Función para tokenizar código y nombres sugeridos con tipo y corrección\"\"\"\n",
    "#     inputs = [\n",
    "#         f\"Generate name [type: {typ}, correct: {corr}]: {code}\"\n",
    "#         for typ, corr, code in zip(\n",
    "#             examples[\"type\"], examples[\"is_correct\"], examples[\"code\"]\n",
    "#         )\n",
    "#     ]\n",
    "#     targets = examples[\"suggested_name\"]\n",
    "\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs, max_length=512, truncation=True, padding=\"max_length\"\n",
    "#     )\n",
    "#     labels = tokenizer(targets, max_length=20, truncation=True, padding=\"max_length\")\n",
    "\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "\n",
    "# # Convertir DataFrame a Dataset de Hugging Face\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# # Aplicar tokenización al dataset\n",
    "# tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# # 3. Dividir en train/test\n",
    "# split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "# train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "# # 4. Cargar el modelo CodeT5\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "# # 5. Configurar entrenamiento\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./codet5-fine-tuned\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     learning_rate=5e-5,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# # 6. Entrenar el modelo\n",
    "# trainer.train()\n",
    "\n",
    "# # 7. Guardar el modelo\n",
    "# model.save_pretrained(\"./codet5-fine-tuned\")\n",
    "# tokenizer.save_pretrained(\"./codet5-fine-tuned\")\n",
    "\n",
    "# print(\"¡Entrenamiento completado y modelo guardado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_correct\"] = df[\"is_correct\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83326 entries, 0 to 83325\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   code            83326 non-null  object\n",
      " 1   suggested_name  83322 non-null  object\n",
      " 2   type            83326 non-null  object\n",
      " 3   is_correct      83326 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code              0\n",
      "suggested_name    4\n",
      "type              0\n",
      "is_correct        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())  # Verifica cuántos valores nulos hay por columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Elimina filas con valores nulos\n",
    "# O\n",
    "# df = df.fillna(\"\")  # Reemplaza nulos con una cadena vacía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code              0\n",
      "suggested_name    0\n",
      "type              0\n",
      "is_correct        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
